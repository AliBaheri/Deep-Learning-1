{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training a skip-gram model in Word2Vec section, the goal of this notebook is to train a LSTM character model over Text8 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295130 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.98\n",
      "================================================================================\n",
      "mkhofgqoxlziebpv oylv jcvjjetsfqhqhpnccxp ufnehtj  olzazwlavj gucdimim rbv lmwlp\n",
      "m   q pqtxbemgtlersowwfmiiylpkloxbolegnzcwhygem n lbm    h i auiemxtfmwteasn  nj\n",
      "igcbol qa  y   kwweql crifei  r supfyi o a psqeia na  hrmfkzsxbdw tc ntwfabt sqf\n",
      "lhr mehhcinmtms deimd ieme v ok rbsqnqaftrm alyml hdxcaryhm oezkhwssixatnwv vm i\n",
      "xjitsaomve uatcmoh tspnjznlarxgf np  ptit inv aw b eqlssou mjlegnze vannrinusypq\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.608005 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.22\n",
      "Validation set perplexity: 10.71\n",
      "Average loss at step 200: 2.264195 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.72\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 300: 2.107036 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 400: 2.004070 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 500: 1.935068 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 600: 1.910239 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 700: 1.861759 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 800: 1.821981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 900: 1.827878 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1000: 1.824702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "quis fefrer of evergy is the concoms were hiveric secopuen kizlis chimpted mage \n",
      " kintans the rames of frecten bretal belikglive and eqpare whise is a ustan seve\n",
      "tions of the the facred leserent in replad no rate sing peteter stegnally evia c\n",
      "wer defile the edwarlm it svans in facaty in and hin the ordatic becing plated s\n",
      "que a forks protzed and adon the vorre one five four the genfenter hovez latue h\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.773523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1200: 1.752735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 1300: 1.734602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1400: 1.745624 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1500: 1.734206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.745444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1700: 1.710617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.676522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1900: 1.646688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2000: 1.696650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "kent thisirely mubbeghs havian this jend didylallof sadenges three on tu product\n",
      "wiblif continuanling instartived that aramber quadli and enso onccists equalally\n",
      "inged bee evencing puppint lime on with as two hab a quinlona gutt the the clost\n",
      "quwars evilt vock as his muding moitweal yached losgy so himplible bratitor the \n",
      "is hodders builmbics in cickongactions ordassing has dodistiy of s the sparohy h\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2100: 1.684442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2200: 1.680727 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.643427 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2400: 1.658922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2500: 1.679461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2600: 1.649904 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2700: 1.659465 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2800: 1.645541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2900: 1.648751 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3000: 1.648758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "calllum can abmerians linctire assivition it by name sood gaining for two five s\n",
      "ed the practien fire busascabe by has matthisl of the carrian jucail dian form h\n",
      "anced rublew j refrecifical grawsivers to abublic and s fromhidench of composing\n",
      "ing by black indiver in they court rumphanderft fears swiscally store invelplate\n",
      "y act give sparnatian these anr otrant former impartain a was gallild indian in \n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.627702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3200: 1.641855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.635720 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3400: 1.666364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500: 1.656131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3600: 1.667635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3700: 1.642600 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3800: 1.645060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3900: 1.637464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4000: 1.649491 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "en je to the fastem to city on apenicial with more been pusious ffo used of a ag\n",
      " which leper kcentitions the box perpons fiect one eight as from fand are notadi\n",
      "zers on vayers americans and word board which witche dups a exempleans zelbid an\n",
      "er three one seince b the many workas be precies writerparsy into sveciets nemch\n",
      "el fillings teld force the protages the mibrerst mudssalt geneations maints acre\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4100: 1.632089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.635775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.616307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4400: 1.609790 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4500: 1.613363 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4600: 1.613209 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4700: 1.626255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4800: 1.628099 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4900: 1.630061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5000: 1.607553 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "jan power fepiorempers caw ene losk office locas a heat baye raintes of the prob\n",
      "wise cripteing difterseine no a midmated of thrings hunting a soned carally hoor\n",
      "urre not shild one nine nine eight two eight viver five three four four sons and\n",
      "minar to gosulf year interverytestaul electurate american to mudden in by liquit\n",
      "in sope has side the some and one nine nine eight eight pnard swanty of the hebb\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5100: 1.605940 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5200: 1.585600 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5300: 1.570447 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5400: 1.576718 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.561886 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5600: 1.576643 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5700: 1.562032 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.574319 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5900: 1.571955 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000: 1.542806 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "er speced jure rame year starger rick of the contrivon in expurpountiamaise and \n",
      "pination of cacced moso or jocelise with penications actions extement shigh me b\n",
      "ings m gpc amobblicies monarch was decthers it a of considered she a tumeres to \n",
      "quifolitber king mairmence for some power holldwage his scotomal the herecidest \n",
      "wartually french which willdow glown was c elected resection three energrachy ca\n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6100: 1.564161 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.529892 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.541933 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.539474 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.552147 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6600: 1.592754 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700: 1.577763 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.598424 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.581998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000: 1.572096 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "tion computer a resuri abou strong mathe it sower mamsic embero that the of b on\n",
      "quage complexion and see begine a english the hill to thiel commor cultures crea\n",
      "mine medilym din by themb in are of the regions jolds pobser to however high it \n",
      "on and percoinerages electrined cherrected transcointics this culture left one o\n",
      "zarians on hous whiston of an aerisimutics and how five ny hold before belit aed\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "Reading carefully [this post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) to understand how LSTM networks operate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM cell state includes the forget gate, input gate, and output gate. The forget gate decides what values are going to be kept or thrown. \n",
    "```\n",
    "f_t = sigmoid(W_f*[h_(t-1), x_i] + b_f) \n",
    "```\n",
    "\n",
    "The input gate decides which values are going to be updated to the next.\n",
    "New values are stored in the cell state.\n",
    "```\n",
    "i_t = sigmoid(W_i*[h_(t-1),x_t]+b_i)\n",
    "/C_t = tanh(W_c*[h_(t-1),x_t]+b_c)\n",
    "```\n",
    "\n",
    "Then, update the old cell state `C_(t-1)` into the new cell state `C_t`:\n",
    "```\n",
    "C_t = f_t*C_(t-1) + i_t*/C_t\n",
    "```\n",
    "\n",
    "The output gate decides what to output to the next cell state.\n",
    "```\n",
    "o_t = sigmoid(W_o*[h_(t-1),x_t]+b_o)\n",
    "h_t = o_t*tanh(C_t)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use single matrix multiplication for LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 256]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # 4 matrix multiplications  \n",
    "    # Gates (f,i,c,o): Forget, input, mem_cell, output \n",
    "    fico_x = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "    print (fico_x.get_shape().as_list())\n",
    "    fico_m = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    fico_b = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"\n",
    "        Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf \n",
    "        Note that in this formulation, we omit the various connections between\n",
    "        the previous state and the gates.\n",
    "        \"\"\"\n",
    "        h_t = tf.matmul(i, fico_x) + tf.matmul(o, fico_m) + fico_b\n",
    "        input_gate = tf.sigmoid(h_t[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(h_t[:,num_nodes:2*num_nodes])\n",
    "        update = tf.tanh(h_t[:,2*num_nodes:3*num_nodes])\n",
    "        state = forget_gate*state + input_gate*update\n",
    "        output_gate = tf.sigmoid(h_t[:,3*num_nodes:])\n",
    "        h = output_gate * tf.tanh(state)\n",
    "        return h, state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=False) # Org: 10.0, 5000, 0.1, True\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300701 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "================================================================================\n",
      "umao tiqev wnnt  m anz d mlohxlsav nalgehx tpzfqetgsanenscjjs   ookbbswpgrl  bxg\n",
      "ew zod yktsgog lldxipkqies doecnw ltfweerq dw nhmcson tefi overfr tcoauqisex eti\n",
      "y  trlukzarykukwgnw atgqosotiw  rf mp zrfesg e rcotry nqeberspn  svosr ardsu  nv\n",
      "xemyxog ljs urws   wys jtxoosekizf z flatlxsscirdesoswxj ygqmgbp mfc lytqwkgmqe \n",
      "kmmniyaitkk  ahr yxjnjbhpf x pdekscjsicerjrqt acucek  qwgs ttbi ht cwkqzen j icp\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.607492 learning rate: 9.549926\n",
      "Minibatch perplexity: 9.49\n",
      "Validation set perplexity: 11.30\n",
      "Average loss at step 200: 2.280010 learning rate: 9.120109\n",
      "Minibatch perplexity: 8.55\n",
      "Validation set perplexity: 9.15\n",
      "Average loss at step 300: 2.128855 learning rate: 8.709636\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 400: 2.017561 learning rate: 8.317638\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 500: 1.954866 learning rate: 7.943282\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 600: 1.919815 learning rate: 7.585776\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 700: 1.890349 learning rate: 7.244360\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 800: 1.900347 learning rate: 6.918310\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 900: 1.856782 learning rate: 6.606935\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 1000: 1.840364 learning rate: 6.309574\n",
      "Minibatch perplexity: 6.51\n",
      "================================================================================\n",
      "ang the acfbuent scousled incepiale inclution lane the was tha i abferstion the \n",
      "ated the deplesual sfivitu and the is topgys pylized milistwo deon one of that i\n",
      "le encineshe yarge izen row fize seners warkee in the deriale the iserament repa\n",
      "ment poasuperammrjogt  ormenat intycuped a keaser in the craald tacbests on the \n",
      "coluturist the begures five sellineerman anbudio s coctaned clie ang the wore re\n",
      "================================================================================\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1100: 1.833676 learning rate: 6.025596\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 1200: 1.813312 learning rate: 5.754399\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1300: 1.795724 learning rate: 5.495409\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.768196 learning rate: 5.248075\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.748239 learning rate: 5.011872\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1600: 1.742804 learning rate: 4.786301\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1700: 1.723275 learning rate: 4.570882\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1800: 1.754371 learning rate: 4.365158\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1900: 1.741522 learning rate: 4.168694\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2000: 1.732926 learning rate: 3.981072\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "s lengon sit is maury of the interson amorggx tecring achivine hever of the dene\n",
      "er as wera starather and dup to aptemir dues otherdefisconds two  in histord of \n",
      "ch of charshis pitrate cont one booed war hists they nava treis haverning exanns\n",
      "rubly incrude of d hildens not tooather compariey scherty to charpame ramelle be\n",
      "ber includerencly to dectrexted profter and thee sists has bray in infantations \n",
      "================================================================================\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2100: 1.716155 learning rate: 3.801894\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2200: 1.714779 learning rate: 3.630781\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2300: 1.705886 learning rate: 3.467369\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2400: 1.685963 learning rate: 3.311311\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2500: 1.693219 learning rate: 3.162278\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2600: 1.699617 learning rate: 3.019952\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2700: 1.689917 learning rate: 2.884031\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2800: 1.679931 learning rate: 2.754229\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2900: 1.658622 learning rate: 2.630268\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.661176 learning rate: 2.511886\n",
      "Minibatch perplexity: 5.64\n",
      "================================================================================\n",
      "ch or glowar adbles the melice st actanoth ding brocbseqenial muse restrocies in\n",
      "mbtish weer const st that sichtel whathers on the singled glatenstedsys opportii\n",
      "questication of griapers have inclued to one nine zero seven four kid its andy o\n",
      "eger batk not rether offegned have of quidele from to ch belizorgers the untriti\n",
      "ve as yussan arpient reportian in offece that well ischille sings the europe hig\n",
      "================================================================================\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3100: 1.682206 learning rate: 2.398833\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 3200: 1.661293 learning rate: 2.290868\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3300: 1.663304 learning rate: 2.187762\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3400: 1.662116 learning rate: 2.089296\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3500: 1.660386 learning rate: 1.995262\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3600: 1.660124 learning rate: 1.905461\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3700: 1.660768 learning rate: 1.819701\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3800: 1.624834 learning rate: 1.737801\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3900: 1.674338 learning rate: 1.659587\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4000: 1.650533 learning rate: 1.584893\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "tly the parting aroup stword to the sordory impanfing the cjiest in and writers \n",
      "que heaged severs the was bees stade to ge groups as it a renooms vas ylavers th\n",
      "th are through sendar halpo one nine eight ambles other is cawniby gever simber \n",
      "ined his crausebul unstazed therse including trighten at the as in repallisonist\n",
      "s so puelly stuch that when the haves suruad set also alouds i meporially an sev\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4100: 1.652821 learning rate: 1.513561\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4200: 1.646041 learning rate: 1.445440\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4300: 1.654308 learning rate: 1.380384\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4400: 1.661170 learning rate: 1.318257\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4500: 1.652217 learning rate: 1.258925\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4600: 1.619958 learning rate: 1.202264\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4700: 1.625720 learning rate: 1.148154\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4800: 1.665032 learning rate: 1.096478\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.643603 learning rate: 1.047129\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 5000: 1.649889 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "jection a deringes and templore hylds the namolary a briond to cart to be in the\n",
      "y on admy insteit sinces yashire all a spict with this adplications the unitary \n",
      "key intlact in the estem acouts which stryines waberaqueft the item to a untrili\n",
      "jach known suiched for is inceneing virtiad and eling five one froignatine of th\n",
      "comained write the rave seven nine two zero three three zero nant in the byts an\n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 5100: 1.660507 learning rate: 0.954993\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 5200: 1.670410 learning rate: 0.912011\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 5300: 1.612089 learning rate: 0.870964\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5400: 1.598203 learning rate: 0.831764\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 5500: 1.651439 learning rate: 0.794328\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 5600: 1.637805 learning rate: 0.758578\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 5700: 1.623006 learning rate: 0.724436\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5800: 1.617884 learning rate: 0.691831\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5900: 1.625455 learning rate: 0.660694\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 6000: 1.662861 learning rate: 0.630957\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      " september partakul war events in is production nat cridves was begarand the sub\n",
      "blatoag limits sernemer one zero zero yeatistly to current peplation flunds unvi\n",
      "onksing org is cournation of fey war bounnd age current desforded lather to for \n",
      "e or the founds culediat league belowase prolong playerapoos of for philization \n",
      "bbeatn subtheten s fathes two zero zero thises the urelarmagy shuch dis the imme\n",
      "================================================================================\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 6100: 1.639358 learning rate: 0.602560\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 6200: 1.636108 learning rate: 0.575440\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 6300: 1.653092 learning rate: 0.549541\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6400: 1.657143 learning rate: 0.524808\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 6500: 1.625059 learning rate: 0.501187\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6600: 1.639313 learning rate: 0.478630\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6700: 1.632834 learning rate: 0.457088\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 6800: 1.677632 learning rate: 0.436516\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 6900: 1.655962 learning rate: 0.416869\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 7000: 1.658109 learning rate: 0.398107\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "warn its maton formance other to the americans stated the large of late effere u\n",
      "bul one nine eight zero nine nine ever with a peased e specition ly if how its i\n",
      "net of sounce lagar see as the recerops and the not greece two zero wave use as \n",
      "baigrak seven one nine nine zero famum of lostics and yin musta he he manday ley\n",
      "ht to billion of clusace is nine sime rignation wost alfame juict with also and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "+ Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "+ Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "+ Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to [this article](http://arxiv.org/abs/1409.2329).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Come back soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "(difficult!)\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "the quick brown fox\n",
    "\n",
    "the model should attempt to output:\n",
    "eht kciuq nworb xof\n",
    "\n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as this [article](http://arxiv.org/abs/1409.3215) for best practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Come back soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
